{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1880e4b6-c05d-49b8-835a-911672cbbe69",
   "metadata": {},
   "source": [
    "## 02 — Preprocessing & Baseline Modeling (Governed)\n",
    "\n",
    "This notebook constructs the **governed preprocessing pipeline** and fits\n",
    "**interpretable baseline models** for the early credit risk warning analysis.\n",
    "\n",
    "All preprocessing and modeling decisions are made **once and only once** in this\n",
    "notebook to ensure methodological consistency, prevent data leakage, and\n",
    "preserve auditability across downstream evaluation.\n",
    "\n",
    "The outputs of this notebook are **frozen modeling artifacts** (preprocessing\n",
    "pipelines, train/test splits, and predicted probabilities) that are saved to\n",
    "disk and consumed unchanged by Notebook 03.\n",
    "\n",
    "---\n",
    "\n",
    "### Objectives\n",
    "\n",
    "- Construct a leak-safe, reproducible preprocessing pipeline  \n",
    "- Explicitly define feature transformations for numeric and categorical variables  \n",
    "- Enforce strict train/test separation prior to any transformation  \n",
    "- Fit interpretable baseline models suitable for early warning contexts  \n",
    "- Persist governed modeling artifacts for downstream evaluation  \n",
    "\n",
    "---\n",
    "\n",
    "### Real-World Usage Context\n",
    "\n",
    "This notebook supports the development of **early credit risk warning models**\n",
    "intended to:\n",
    "\n",
    "- identify accounts requiring closer monitoring,  \n",
    "- support manual review and prioritization,  \n",
    "- inform proportionate, preventative interventions.  \n",
    "\n",
    "Model outputs are **probabilistic risk signals**, not automated decisions, and\n",
    "are not used for credit approval, rejection, or pricing.\n",
    "\n",
    "---\n",
    "\n",
    "### Governance Principles\n",
    "\n",
    "- **No data leakage**: preprocessing is fit on training data only via pipelines  \n",
    "- **Single source of truth**: all inputs originate from governed artifacts\n",
    "  produced in Notebook 01  \n",
    "- **Interpretability by design**: logistic regression baselines are prioritized  \n",
    "- **Separation of concerns**: evaluation and benchmarking are deferred to\n",
    "  Notebook 03  \n",
    "- **Reproducibility**: all randomness is seeded and all outputs are persisted  \n",
    "\n",
    "---\n",
    "\n",
    "### Scope and Limitations\n",
    "\n",
    "The models fitted in this notebook are **baseline reference models**, not\n",
    "production-ready systems.\n",
    "\n",
    "They are intended to:\n",
    "- validate signal presence,  \n",
    "- establish transparent benchmarks,  \n",
    "- support methodological discussion.  \n",
    "\n",
    "Performance optimization, threshold selection, and benchmark comparisons are\n",
    "handled in the subsequent notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704c6d0a-df51-4ac3-a53c-7983b60945d0",
   "metadata": {},
   "source": [
    "#### Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8b032e3-144b-4196-8b1c-3394e5f86d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective:\n",
    "# Ensure a clean, reproducible environment and load all dependencies required\n",
    "# for data preparation, modeling, and diagnostics.\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db167963-f954-48e9-80b2-9001d5f01fe4",
   "metadata": {},
   "source": [
    "#### Governed data loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d72e26c3-dc9d-45bb-ac50-8f253fb448af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figures will be saved to: /Users/steph/Desktop/Github/early-warning-credit-risk/03_artifacts/notebook02/figures\n"
     ]
    }
   ],
   "source": [
    "# Paths (governed artifacts only)\n",
    "NOTEBOOK_DIR = Path(\"/Users/steph/Desktop/Github/early-warning-credit-risk/02_notebooks\")\n",
    "PROJECT_ROOT = NOTEBOOK_DIR.parent\n",
    "\n",
    "IN_DIR = PROJECT_ROOT / \"03_artifacts\" / \"notebook01\"\n",
    "OUT_DIR = PROJECT_ROOT / \"03_artifacts\" / \"notebook02\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "FIG_DIR = OUT_DIR / \"figures\"\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Figures will be saved to:\", FIG_DIR.resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cef2ef-a473-42bc-9e37-c263cbfc0d0c",
   "metadata": {},
   "source": [
    "#### Governance principle:\n",
    "- This notebook must only consume artifacts produced upstream\n",
    "- Raw data files are never reloaded at this stage\n",
    "- Target definition and column structure are considered frozen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4f32850-c58d-4aaa-9e7d-015e2eb4f735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded governed dataset\n",
      "Shape: (1000, 22)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status_checking_account</th>\n",
       "      <th>duration_months</th>\n",
       "      <th>credit_history</th>\n",
       "      <th>purpose</th>\n",
       "      <th>credit_amount</th>\n",
       "      <th>savings_account</th>\n",
       "      <th>employment_since</th>\n",
       "      <th>installment_rate</th>\n",
       "      <th>personal_status_sex</th>\n",
       "      <th>other_debtors</th>\n",
       "      <th>...</th>\n",
       "      <th>age</th>\n",
       "      <th>other_installment_plans</th>\n",
       "      <th>housing</th>\n",
       "      <th>existing_credits</th>\n",
       "      <th>job</th>\n",
       "      <th>num_dependents</th>\n",
       "      <th>telephone</th>\n",
       "      <th>foreign_worker</th>\n",
       "      <th>credit_risk</th>\n",
       "      <th>y_bad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A11</td>\n",
       "      <td>6</td>\n",
       "      <td>A34</td>\n",
       "      <td>A43</td>\n",
       "      <td>1169</td>\n",
       "      <td>A65</td>\n",
       "      <td>A75</td>\n",
       "      <td>4</td>\n",
       "      <td>A93</td>\n",
       "      <td>A101</td>\n",
       "      <td>...</td>\n",
       "      <td>67</td>\n",
       "      <td>A143</td>\n",
       "      <td>A152</td>\n",
       "      <td>2</td>\n",
       "      <td>A173</td>\n",
       "      <td>1</td>\n",
       "      <td>A192</td>\n",
       "      <td>A201</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A12</td>\n",
       "      <td>48</td>\n",
       "      <td>A32</td>\n",
       "      <td>A43</td>\n",
       "      <td>5951</td>\n",
       "      <td>A61</td>\n",
       "      <td>A73</td>\n",
       "      <td>2</td>\n",
       "      <td>A92</td>\n",
       "      <td>A101</td>\n",
       "      <td>...</td>\n",
       "      <td>22</td>\n",
       "      <td>A143</td>\n",
       "      <td>A152</td>\n",
       "      <td>1</td>\n",
       "      <td>A173</td>\n",
       "      <td>1</td>\n",
       "      <td>A191</td>\n",
       "      <td>A201</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A14</td>\n",
       "      <td>12</td>\n",
       "      <td>A34</td>\n",
       "      <td>A46</td>\n",
       "      <td>2096</td>\n",
       "      <td>A61</td>\n",
       "      <td>A74</td>\n",
       "      <td>2</td>\n",
       "      <td>A93</td>\n",
       "      <td>A101</td>\n",
       "      <td>...</td>\n",
       "      <td>49</td>\n",
       "      <td>A143</td>\n",
       "      <td>A152</td>\n",
       "      <td>1</td>\n",
       "      <td>A172</td>\n",
       "      <td>2</td>\n",
       "      <td>A191</td>\n",
       "      <td>A201</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A11</td>\n",
       "      <td>42</td>\n",
       "      <td>A32</td>\n",
       "      <td>A42</td>\n",
       "      <td>7882</td>\n",
       "      <td>A61</td>\n",
       "      <td>A74</td>\n",
       "      <td>2</td>\n",
       "      <td>A93</td>\n",
       "      <td>A103</td>\n",
       "      <td>...</td>\n",
       "      <td>45</td>\n",
       "      <td>A143</td>\n",
       "      <td>A153</td>\n",
       "      <td>1</td>\n",
       "      <td>A173</td>\n",
       "      <td>2</td>\n",
       "      <td>A191</td>\n",
       "      <td>A201</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A11</td>\n",
       "      <td>24</td>\n",
       "      <td>A33</td>\n",
       "      <td>A40</td>\n",
       "      <td>4870</td>\n",
       "      <td>A61</td>\n",
       "      <td>A73</td>\n",
       "      <td>3</td>\n",
       "      <td>A93</td>\n",
       "      <td>A101</td>\n",
       "      <td>...</td>\n",
       "      <td>53</td>\n",
       "      <td>A143</td>\n",
       "      <td>A153</td>\n",
       "      <td>2</td>\n",
       "      <td>A173</td>\n",
       "      <td>2</td>\n",
       "      <td>A191</td>\n",
       "      <td>A201</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  status_checking_account  duration_months credit_history purpose  \\\n",
       "0                     A11                6            A34     A43   \n",
       "1                     A12               48            A32     A43   \n",
       "2                     A14               12            A34     A46   \n",
       "3                     A11               42            A32     A42   \n",
       "4                     A11               24            A33     A40   \n",
       "\n",
       "   credit_amount savings_account employment_since  installment_rate  \\\n",
       "0           1169             A65              A75                 4   \n",
       "1           5951             A61              A73                 2   \n",
       "2           2096             A61              A74                 2   \n",
       "3           7882             A61              A74                 2   \n",
       "4           4870             A61              A73                 3   \n",
       "\n",
       "  personal_status_sex other_debtors  ...  age other_installment_plans  \\\n",
       "0                 A93          A101  ...   67                    A143   \n",
       "1                 A92          A101  ...   22                    A143   \n",
       "2                 A93          A101  ...   49                    A143   \n",
       "3                 A93          A103  ...   45                    A143   \n",
       "4                 A93          A101  ...   53                    A143   \n",
       "\n",
       "   housing existing_credits   job  num_dependents telephone  foreign_worker  \\\n",
       "0     A152                2  A173               1      A192            A201   \n",
       "1     A152                1  A173               1      A191            A201   \n",
       "2     A152                1  A172               2      A191            A201   \n",
       "3     A153                1  A173               2      A191            A201   \n",
       "4     A153                2  A173               2      A191            A201   \n",
       "\n",
       "  credit_risk y_bad  \n",
       "0           1     0  \n",
       "1           2     1  \n",
       "2           1     0  \n",
       "3           1     0  \n",
       "4           2     1  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "DATA_PATH = IN_DIR / \"german_credit_with_y_bad.csv\"\n",
    "\n",
    "# Fail fast if the governed artifact is missing.\n",
    "# This prevents silent fallbacks and ensures reproducibility.\n",
    "assert DATA_PATH.exists(), (\n",
    "    f\"Governed dataset not found at {DATA_PATH}. \"\n",
    "    \"Notebook 01 must be re-run to regenerate artifacts.\"\n",
    ")\n",
    "\n",
    "# Load governed dataset\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# Minimal structural validation\n",
    "assert \"y_bad\" in df.columns, (\n",
    "    \"Missing event label 'y_bad'. \"\n",
    "    \"Target definition is not governed or dataset is inconsistent.\"\n",
    ")\n",
    "\n",
    "print(\"Loaded governed dataset\")\n",
    "print(\"Shape:\", df.shape)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f71577be-d0f1-4dbd-9761-814e9e8b0045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "status_checking_account    object\n",
       "duration_months             int64\n",
       "credit_history             object\n",
       "purpose                    object\n",
       "credit_amount               int64\n",
       "savings_account            object\n",
       "employment_since           object\n",
       "installment_rate            int64\n",
       "personal_status_sex        object\n",
       "other_debtors              object\n",
       "residence_since             int64\n",
       "property                   object\n",
       "age                         int64\n",
       "other_installment_plans    object\n",
       "housing                    object\n",
       "existing_credits            int64\n",
       "job                        object\n",
       "num_dependents              int64\n",
       "telephone                  object\n",
       "foreign_worker             object\n",
       "credit_risk                 int64\n",
       "y_bad                       int64\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Columns:\")\n",
    "display(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c08833-a0bd-41a6-ac78-288bb86ccefe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84118842-a69e-4da5-b7ab-548b3dc58581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excluded columns: ['y_bad', 'credit_risk']\n",
      "X shape: (1000, 20)\n",
      "y distribution:\n",
      "y_bad\n",
      "0    700\n",
      "1    300\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Define target and exclude leakage-prone columns\n",
    "TARGET_COL = \"y_bad\"\n",
    "\n",
    "EXCLUDE_COLS = [TARGET_COL]\n",
    "\n",
    "# Exclude any raw label columns to prevent leakage\n",
    "for raw_target in [\"credit_risk\", \"target\"]:\n",
    "    if raw_target in df.columns:\n",
    "        EXCLUDE_COLS.append(raw_target)\n",
    "\n",
    "X = df.drop(columns=EXCLUDE_COLS)\n",
    "y = df[TARGET_COL].astype(int)\n",
    "\n",
    "print(\"Excluded columns:\", EXCLUDE_COLS)\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y distribution:\")\n",
    "print(y.value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6448e8-2465-424d-9a75-6f2795d371af",
   "metadata": {},
   "source": [
    "#### Feature typing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5de73577-b66f-4e52-a61c-e5672ba62411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns: 10\n",
      "Numeric columns: 10\n"
     ]
    }
   ],
   "source": [
    "categorical_cols = [\n",
    "    c for c in [\n",
    "        \"status_checking\",\"credit_history\",\"purpose\",\"savings\",\"employment\",\n",
    "        \"personal_status_sex\",\"other_debtors\",\"property\",\"other_installment_plans\",\n",
    "        \"housing\",\"job\",\"telephone\",\"foreign_worker\"\n",
    "    ]\n",
    "    if c in X.columns\n",
    "]\n",
    "\n",
    "numeric_cols = [c for c in X.columns if c not in categorical_cols]\n",
    "\n",
    "print(\"Categorical columns:\", len(categorical_cols))\n",
    "print(\"Numeric columns:\", len(numeric_cols))\n",
    "\n",
    "# Governance note: prevent misclassification of coded categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3ea269f-2704-4b91-9bd6-728ae8fe91f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leakage safeguards\n",
    "assert \"target\" not in X.columns, \"Leakage risk: 'target' still in features\"\n",
    "assert \"credit_risk\" not in X.columns, \"Leakage risk: 'credit_risk' still in features\"\n",
    "assert \"y_bad\" not in X.columns, \"Leakage risk: 'y_bad' still in features\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9838afce-ac2a-404b-a780-9065c71b99fb",
   "metadata": {},
   "source": [
    "#### Train/test split (stratified, reproducible)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f41e99f-7bba-4877-8717-7f5fad7dd14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (800, 20) | Bad rate: 0.3\n",
      "Test  shape: (200, 20) | Bad rate: 0.3\n"
     ]
    }
   ],
   "source": [
    "# Train/Test split\n",
    "RANDOM_STATE = 2712\n",
    "TEST_SIZE = 0.20\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train shape:\", X_train.shape, \"| Bad rate:\", round(y_train.mean(), 4))\n",
    "print(\"Test  shape:\", X_test.shape,  \"| Bad rate:\", round(y_test.mean(), 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4315e710-e60f-4181-be8f-1b1f231301ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns: ['status_checking_account', 'credit_history', 'purpose', 'savings_account', 'employment_since', 'personal_status_sex', 'other_debtors', 'property', 'other_installment_plans', 'housing', 'job', 'telephone', 'foreign_worker']\n",
      "Numeric columns: ['duration_months', 'credit_amount', 'installment_rate', 'residence_since', 'age', 'existing_credits', 'num_dependents']\n"
     ]
    }
   ],
   "source": [
    "# Override any previous lists: use dtype-based typing (robust to coded categories)\n",
    "categorical_cols = X_train.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "numeric_cols = X_train.select_dtypes(exclude=[\"object\"]).columns.tolist()\n",
    "\n",
    "print(\"Categorical columns:\", categorical_cols)\n",
    "print(\"Numeric columns:\", numeric_cols)\n",
    "\n",
    "# Safety: ensure no overlap\n",
    "assert set(categorical_cols).isdisjoint(set(numeric_cols))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89a7aa7d-fb90-4c0f-8061-950ee943189d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class balance check (stratification integrity)\n",
    "assert abs(y_train.mean() - y.mean()) < 0.01, \"Train bad-rate drift\"\n",
    "assert abs(y_test.mean() - y.mean()) < 0.01, \"Test bad-rate drift\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0d302c4-71ab-4522-bf8c-878ebb689bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Principles:\n",
    "# - All transformations are learned on TRAIN only (via Pipeline.fit on X_train)\n",
    "# - Numeric and categorical features are handled separately and explicitly\n",
    "# - Outputs are deterministic and reproducible across notebooks\n",
    "#\n",
    "# Rationale (risk / governance):\n",
    "# - Missing data handling is standardized (no ad-hoc fixes later)\n",
    "# - Scaling is applied for coefficient stability in logistic regression\n",
    "# - One-hot encoding avoids imposing an artificial order on categories\n",
    "# - Unknown categories in test/production are handled safely (no crashes)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Numeric preprocessing: impute missing values + standardize scale\n",
    "numeric_pipeline = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),   # robust to outliers\n",
    "    (\"scaler\", StandardScaler()),                    # improves numerical stability\n",
    "])\n",
    "\n",
    "# Categorical preprocessing: impute missing categories + one-hot encode\n",
    "categorical_pipeline = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),  # stable default for categories\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),    # prevents failure on unseen categories\n",
    "])\n",
    "\n",
    "# Combine into a single preprocessor applied consistently to train/test\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_pipeline, numeric_cols),\n",
    "        (\"cat\", categorical_pipeline, categorical_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",  # governance: only approved feature sets are used\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59408985-656b-4379-840b-c717d0601c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models fitted: logistic, l1_logistic\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Logistic regression baseline (interpretable)\n",
    "log_reg = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"model\", LogisticRegression(max_iter=2000))\n",
    "])\n",
    "\n",
    "# L1-regularized logistic (sparser, feature stability)\n",
    "l1_log_reg = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"model\", LogisticRegression(\n",
    "        penalty=\"l1\",\n",
    "        solver=\"saga\",\n",
    "        max_iter=4000\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Fit on training data only\n",
    "log_reg.fit(X_train, y_train)\n",
    "l1_log_reg.fit(X_train, y_train)\n",
    "\n",
    "print(\"Models fitted: logistic, l1_logistic\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7fdfd6db-56d5-43c8-b74f-52fa74315131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Probability generation (governed outputs)\n",
    "\n",
    "# Governance note:\n",
    "# - Probabilities are saved unchanged\n",
    "# - No thresholding or performance evaluation occurs here\n",
    "# - This prevents analytical drift between modeling and evaluation\n",
    "\n",
    "def predict_proba(pipe, X):\n",
    "    \"\"\"Return event probability P(y_bad = 1).\"\"\"\n",
    "    return pipe.predict_proba(X)[:, 1]\n",
    "\n",
    "# Training-set probabilities (for diagnostics only)\n",
    "train_predictions = pd.DataFrame({\n",
    "    \"y\": y_train,\n",
    "    \"p_logistic\": predict_proba(log_reg, X_train),\n",
    "    \"p_l1_logistic\": predict_proba(l1_log_reg, X_train),\n",
    "})\n",
    "\n",
    "# Test-set probabilities (used for final evaluation)\n",
    "test_predictions = pd.DataFrame({\n",
    "    \"y\": y_test,\n",
    "    \"p_logistic\": predict_proba(log_reg, X_test),\n",
    "    \"p_l1_logistic\": predict_proba(l1_log_reg, X_test),\n",
    "})\n",
    "\n",
    "# Persist predictions for downstream evaluation (Notebook 03)\n",
    "train_predictions.to_csv(OUT_DIR / \"train_predictions.csv\", index=True)\n",
    "test_predictions.to_csv(OUT_DIR / \"test_predictions.csv\", index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b1417cb-593d-4bbb-9d7d-697d1effaf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba(pipe, X):\n",
    "    return pipe.predict_proba(X)[:, 1]\n",
    "\n",
    "train_predictions = pd.DataFrame({\n",
    "    \"y\": y_train,\n",
    "    \"p_logistic\": predict_proba(log_reg, X_train),\n",
    "    \"p_l1_logistic\": predict_proba(l1_log_reg, X_train),\n",
    "})\n",
    "\n",
    "test_predictions = pd.DataFrame({\n",
    "    \"y\": y_test,\n",
    "    \"p_logistic\": predict_proba(log_reg, X_test),\n",
    "    \"p_l1_logistic\": predict_proba(l1_log_reg, X_test),\n",
    "})\n",
    "\n",
    "train_predictions.to_csv(OUT_DIR / \"train_predictions.csv\", index=True)\n",
    "test_predictions.to_csv(OUT_DIR / \"test_predictions.csv\", index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5150db6f-7ca2-440f-90a8-69e5ee659c03",
   "metadata": {},
   "source": [
    "#### Artifacts saving for Notebook 03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b043141a-e10e-4300-bed4-bbb4211ff986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved fitted pipelines.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "joblib.dump(log_reg, OUT_DIR / \"pipeline_logistic.joblib\")\n",
    "joblib.dump(l1_log_reg, OUT_DIR / \"pipeline_l1_logistic.joblib\")\n",
    "\n",
    "print(\"Saved fitted pipelines.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4018a2bc-2a4c-4143-a4aa-2faf2b962744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved governed splits.\n"
     ]
    }
   ],
   "source": [
    "X_train.to_csv(OUT_DIR / \"X_train.csv\", index=True)\n",
    "X_test.to_csv(OUT_DIR / \"X_test.csv\", index=True)\n",
    "y_train.to_csv(OUT_DIR / \"y_train.csv\", index=True)\n",
    "y_test.to_csv(OUT_DIR / \"y_test.csv\", index=True)\n",
    "\n",
    "print(\"Saved governed splits.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4b4bfbe-058d-4638-8be6-3e49ea52e198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved run metadata.\n"
     ]
    }
   ],
   "source": [
    "metadata = {\n",
    "    \"notebook\": \"02_preprocessing_modeling.ipynb\",\n",
    "    \"random_state\": RANDOM_STATE,\n",
    "    \"test_size\": TEST_SIZE,\n",
    "    \"excluded_columns\": EXCLUDE_COLS,\n",
    "    \"n_train\": int(X_train.shape[0]),\n",
    "    \"n_test\": int(X_test.shape[0]),\n",
    "    \"bad_rate_train\": float(y_train.mean()),\n",
    "    \"bad_rate_test\": float(y_test.mean()),\n",
    "    \"numeric_columns\": numeric_cols,\n",
    "    \"categorical_columns\": categorical_cols,\n",
    "    \"models\": [\"logistic_regression\", \"l1_logistic_regression\"],\n",
    "    \"notes\": \"Preprocessing + interpretable baseline models only. Evaluation and Random Forest benchmark in Notebook 03.\"\n",
    "}\n",
    "\n",
    "with open(OUT_DIR / \"run_metadata.json\", \"w\") as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(\"Saved run metadata.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581b9b41-5771-414f-a493-42314ad9835f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
